{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.py\n",
    "\n",
    "OW_API_URL=\"http://api.openweathermap.org/geo/1.0/zip?zip={zip},US&appid={api_key}\"\n",
    "API_KEY=\"c603cf27fe72100c8d3aaf7d7a409c86\"\n",
    "\n",
    "ZILLOW_HOMES_URL=\"https://www.zillow.com/{city}-{state}/{property_type}/{page}_p\"\n",
    "\n",
    "HOME_HEADER_CLASS=(\"div\",{\"class\":\"StyledPropertyCardDataWrapper-c11n-8-85-1__sc-1omp4c3-0 jVBMsP property-card-data\"})\n",
    "HOME_PAGE_CLASS=(\"span\",{\"class\":\"Text-c11n-8-85-1__sc-aiai24-0 bEkett\"})\n",
    "HOME_PRICE_CLASS=(\"div\",{\"class\":\"StyledPropertyCardDataArea-c11n-8-85-1__sc-yipmu-0 bqsBln\"})\n",
    "HOME_SPACE_CLASS=(\"ul\",{\"class\":\"StyledPropertyCardHomeDetailsList-c11n-8-85-1__sc-1xvdaej-0 dmDolk\"})\n",
    "HOME_ADDRESS_CLASS=(\"a\",{\"class\":\"StyledPropertyCardDataArea-c11n-8-85-1__sc-yipmu-0 gdfTyO property-card-link\"})\n",
    "\n",
    "LIST_ITEM_CLASS=\"li\"\n",
    "LIST_ITEM_IDENTIFIER=\"abbr\"\n",
    "\n",
    "ERROR_404_CONSTANT='zillow-error-page'\n",
    "BOT_CAUGHT_CONSTANT='robots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_states=[('Houston','TX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31944/2719036615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/{num_var5}.36 (KHTML, like Gecko) Chrome/51.{num_var2}.2704.{num_var} Safari/537.{num_var3} OPR/38.0.{num_var4}.41\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             req = requests.get(ZILLOW_HOMES_URL.format(city=city.lower(),\n\u001b[0m\u001b[0;32m     26\u001b[0m                                                         \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                                                         \u001b[0mproperty_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'homes'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    527\u001b[0m         }\n\u001b[0;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 resp = self.send(\n\u001b[0m\u001b[0;32m    238\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rsher\\anaconda3\\envs\\scrape_env\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main.py\n",
    "\n",
    "l=[]\n",
    "h={}\n",
    "\n",
    "for city, state in city_states:  \n",
    "    while True:\n",
    "        num_var=np.random.randint(100,1000)\n",
    "        num_var3=np.random.randint(10,100)\n",
    "        num_var2=num_var3%10\n",
    "        num_var4=np.random.randint(1000,10000)\n",
    "        num_var5=np.random.randint(100,1000)\n",
    "\n",
    "        _header={\"User-Agent\": f\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/{num_var5}.36 (KHTML, like Gecko) Chrome/51.{num_var2}.2704.{num_var} Safari/537.{num_var3} OPR/38.0.{num_var4}.41\"}\n",
    "        _page_req=requests.get(ZILLOW_HOMES_URL.format(city=city.lower(),\n",
    "                                                        state=state.lower(),\n",
    "                                                        property_type='homes',\n",
    "                                                        page='2'),headers=_header)\n",
    "\n",
    "        _page_text = _page_req.text\n",
    "\n",
    "        _page_soup=BeautifulSoup(_page_text,'html.parser')\n",
    "\n",
    "        try:\n",
    "            if _page_soup.find('div')['id']==ERROR_404_CONSTANT:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if _page_soup.find('meta')['name']==BOT_CAUGHT_CONSTANT:\n",
    "            continue\n",
    "        else:\n",
    "            if _page_soup.find(HOME_PAGE_CLASS[0],HOME_PAGE_CLASS[1]) == None:\n",
    "                _page_max=1\n",
    "            else:\n",
    "                _target_text=_page_soup.find(HOME_PAGE_CLASS[0],HOME_PAGE_CLASS[1]).text\n",
    "\n",
    "                _page_max=int(re.findall('(\\d+)',_target_text)[1])\n",
    "            break\n",
    "\n",
    "\n",
    "    for p_idx in range(2,(_page_max+2)):\n",
    "        try:\n",
    "            if _page_soup.find('div')['id']==ERROR_404_CONSTANT:\n",
    "                break\n",
    "        except:\n",
    "                pass\n",
    "            \n",
    "        while True:\n",
    "            num_var=np.random.randint(100,1000)\n",
    "            num_var3=np.random.randint(10,100)\n",
    "            num_var2=num_var3%10\n",
    "            num_var4=np.random.randint(1000,10000)\n",
    "            num_var5=np.random.randint(100,1000)\n",
    "\n",
    "            header={\"User-Agent\": f\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/{num_var5}.36 (KHTML, like Gecko) Chrome/51.{num_var2}.2704.{num_var} Safari/537.{num_var3} OPR/38.0.{num_var4}.41\"}\n",
    "            req = requests.get(ZILLOW_HOMES_URL.format(city=city.lower(),\n",
    "                                                        state=state.lower(),\n",
    "                                                        property_type='homes',\n",
    "                                                        page=p_idx),headers=header)\n",
    "            \n",
    "            r_text=req.text\n",
    "\n",
    "\n",
    "            soup = BeautifulSoup(r_text,'html.parser')\n",
    "\n",
    "\n",
    "            if soup.find('meta')['name']==BOT_CAUGHT_CONSTANT:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "\n",
    "                homes = soup.find_all(HOME_HEADER_CLASS[0],HOME_HEADER_CLASS[1])\n",
    "\n",
    "                for home in homes:\n",
    "\n",
    "                    p_list=home.find(HOME_PRICE_CLASS[0],HOME_PRICE_CLASS[1]).text\n",
    "                    try:\n",
    "                        h['price']=int(\"\".join(re.findall('(\\d+)',p_list)))\n",
    "                    except:\n",
    "                        h['price']=None\n",
    "\n",
    "\n",
    "                    s_list=home.find(HOME_SPACE_CLASS[0],HOME_SPACE_CLASS[1])\n",
    "                    try:\n",
    "                        s_list_list = s_list.find_all('li')\n",
    "                        for li_idx in range(len(s_list_list)):\n",
    "                            li_identifier = s_list_list[li_idx].find('abbr')  \n",
    "                            try:\n",
    "                                if re.findall(\"(bd)\", li_identifier.text)[0] == 'bd': \n",
    "                                    h['bd'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                            try:\n",
    "                                if re.findall(\"(ba)\", li_identifier.text)[0] == 'ba':  \n",
    "                                    h['ba'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "                            try:\n",
    "                                if re.findall(\"(sqft)\", li_identifier.text)[0] == 'sqft':\n",
    "                                    h['sqft'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                            except:\n",
    "                                pass\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    a_list=home.find(HOME_ADDRESS_CLASS[0],HOME_ADDRESS_CLASS[1]).text.split(\",\")\n",
    "                    try:\n",
    "                        h['address']=a_list[0].strip()\n",
    "                    except:\n",
    "                        h['address']=None\n",
    "                    try:\n",
    "                        h['city']=a_list[1].strip()\n",
    "                    except:\n",
    "                        h['city']=None\n",
    "                    try:\n",
    "                        h['state']=a_list[2].strip().split()[0]\n",
    "                    except:\n",
    "                        h['state']=None\n",
    "                    try:\n",
    "                        h['zip']=a_list[2].strip().split()[1]\n",
    "                    except:\n",
    "                        h['zip']=None\n",
    "\n",
    "\n",
    "                    header={\"content-type\": \"application/json\"}\n",
    "                    req=requests.get(OW_API_URL.format(zip=h['zip'],api_key=API_KEY),headers=header)\n",
    "\n",
    "                    json_loads=json.loads(req.text)\n",
    "\n",
    "                    try:\n",
    "                        h['lat']=float(json_loads['lat'])\n",
    "                        h['lon']=float(json_loads['lon'])\n",
    "                    except:\n",
    "                        h['lat']=None\n",
    "                        h['lon']=None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    l.append(h)\n",
    "                    h={}\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onwards and Upwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "dict={'zillow_call':[],'structure_data':[],'openweather_call':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "\n",
    "    tic=time.perf_counter()\n",
    "    num_var=np.random.randint(100,1000)\n",
    "    num_var3=np.random.randint(10,100)\n",
    "    num_var2=num_var3%10\n",
    "    num_var4=np.random.randint(1000,10000)\n",
    "    num_var5=np.random.randint(100,1000)\n",
    "\n",
    "    _header={\"User-Agent\": f\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/{num_var5}.36 (KHTML, like Gecko) Chrome/51.{num_var2}.2704.{num_var} Safari/537.{num_var3} OPR/38.0.{num_var4}.41\"}\n",
    "    _page_req=requests.get(ZILLOW_HOMES_URL.format(city=city.lower(),\n",
    "                                                    state=state.lower(),\n",
    "                                                    property_type='homes',\n",
    "                                                    page='2'),headers=_header)\n",
    "\n",
    "    _page_text = _page_req.text\n",
    "\n",
    "    page_soup=BeautifulSoup(_page_text,'html.parser')\n",
    "\n",
    "    toc=time.perf_counter()\n",
    "\n",
    "    dict['zillow_call'].append(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "\n",
    "    tic=time.perf_counter()\n",
    "\n",
    "    homes = soup.find_all(HOME_HEADER_CLASS[0],HOME_HEADER_CLASS[1])\n",
    "\n",
    "    for home in homes:\n",
    "\n",
    "        p_list=home.find(HOME_PRICE_CLASS[0],HOME_PRICE_CLASS[1]).text\n",
    "        try:\n",
    "            h['price']=int(\"\".join(re.findall('(\\d+)',p_list)))\n",
    "        except:\n",
    "            h['price']=None\n",
    "\n",
    "\n",
    "        s_list=home.find(HOME_SPACE_CLASS[0],HOME_SPACE_CLASS[1])\n",
    "        try:\n",
    "            s_list_list = s_list.find_all('li')\n",
    "            for li_idx in range(len(s_list_list)):\n",
    "                li_identifier = s_list_list[li_idx].find('abbr')  \n",
    "                try:\n",
    "                    if re.findall(\"(bd)\", li_identifier.text)[0] == 'bd': \n",
    "                        h['bd'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "                try:\n",
    "                    if re.findall(\"(ba)\", li_identifier.text)[0] == 'ba':  \n",
    "                        h['ba'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    if re.findall(\"(sqft)\", li_identifier.text)[0] == 'sqft':\n",
    "                        h['sqft'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        a_list=home.find(HOME_ADDRESS_CLASS[0],HOME_ADDRESS_CLASS[1]).text.split(\",\")\n",
    "        try:\n",
    "            h['address']=a_list[0].strip()\n",
    "        except:\n",
    "            h['address']=None\n",
    "        try:\n",
    "            h['city']=a_list[1].strip()\n",
    "        except:\n",
    "            h['city']=None\n",
    "        try:\n",
    "            h['state']=a_list[2].strip().split()[0]\n",
    "        except:\n",
    "            h['state']=None\n",
    "        try:\n",
    "            h['zip']=a_list[2].strip().split()[1]\n",
    "        except:\n",
    "            h['zip']=None\n",
    "\n",
    "    toc=time.perf_counter()\n",
    "\n",
    "    dict['structure_data'].append(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):    \n",
    "    tic=time.perf_counter()\n",
    "\n",
    "    header={\"content-type\": \"application/json\"}\n",
    "    req=requests.get(OW_API_URL.format(zip=h['zip'],api_key=API_KEY),headers=header)\n",
    "\n",
    "    json_loads=json.loads(req.text)\n",
    "\n",
    "    try:\n",
    "        h['lat']=float(json_loads['lat'])\n",
    "        h['lon']=float(json_loads['lon'])\n",
    "    except:\n",
    "        h['lat']=None\n",
    "        h['lon']=None\n",
    "\n",
    "    toc=time.perf_counter()\n",
    "\n",
    "    dict['openweather_call'].append(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868758181000012\n",
      "0.00391151899995748\n",
      "0.1322673790004046\n"
     ]
    }
   ],
   "source": [
    "def mean(list):\n",
    "    mean=sum(list)/len(list)\n",
    "    return mean\n",
    "\n",
    "print(mean(dict['zillow_call']))\n",
    "print(mean(dict['structure_data']))\n",
    "print(mean(dict['openweather_call']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import time\n",
    "\n",
    "def mean(list):\n",
    "    mean=sum(list)/len(list)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = {}\n",
    "\n",
    "def memoize_zip_codes(func):\n",
    "    def wrapper(dict):\n",
    "        if dict['zip'] not in memory:\n",
    "            json_loads=func(dict) # CALL OW_api_call\n",
    "            try:\n",
    "                memory[dict['zip']]={'lat':json_loads['lat'],\"lon\":json_loads['lon']} # ADD NOVEL ZIP CODE TO memory\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            dict['lat']=memory[dict['zip']]['lat'] # GET ZIP CODE COORDINATES FROM memory\n",
    "            dict['lon']=memory[dict['zip']]['lon']\n",
    "        return func\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize_zip_codes\n",
    "def OW_api_call(dict):\n",
    "    header={\"content-type\": \"application/json\"}\n",
    "    req=requests.get(OW_API_URL.format(zip=dict['zip'],api_key=API_KEY),headers=header)\n",
    "\n",
    "    json_loads=json.loads(req.text)\n",
    "\n",
    "    try:\n",
    "        dict['lat']=float(json_loads['lat'])\n",
    "        dict['lon']=float(json_loads['lon'])\n",
    "        \n",
    "\n",
    "    except:\n",
    "        dict['lat']=None\n",
    "        dict['lon']=None\n",
    "\n",
    "    return json_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "h={}\n",
    "\n",
    "for city, state in city_states:  \n",
    "    while True:\n",
    "        num_var=np.random.randint(100,1000)\n",
    "        num_var3=np.random.randint(10,100)\n",
    "        num_var2=num_var3%10\n",
    "        num_var4=np.random.randint(1000,10000)\n",
    "        num_var5=np.random.randint(100,1000)\n",
    "\n",
    "        _header={\"User-Agent\": f\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/{num_var5}.36 (KHTML, like Gecko) Chrome/51.{num_var2}.2704.{num_var} Safari/537.{num_var3} OPR/38.0.{num_var4}.41\"}\n",
    "        _page_req=requests.get(ZILLOW_HOMES_URL.format(city=city.lower(),\n",
    "                                                        state=state.lower(),\n",
    "                                                        property_type='homes',\n",
    "                                                        page='2'),headers=_header)\n",
    "\n",
    "        _page_text = _page_req.text\n",
    "\n",
    "        _page_soup=BeautifulSoup(_page_text,'html.parser')\n",
    "\n",
    "        try:\n",
    "            if _page_soup.find('div')['id']==ERROR_404_CONSTANT:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if _page_soup.find('meta')['name']==BOT_CAUGHT_CONSTANT:\n",
    "            continue\n",
    "        else:\n",
    "            if _page_soup.find(HOME_PAGE_CLASS[0],HOME_PAGE_CLASS[1]) == None:\n",
    "                _page_max=1\n",
    "            else:\n",
    "                _target_text=_page_soup.find(HOME_PAGE_CLASS[0],HOME_PAGE_CLASS[1]).text\n",
    "\n",
    "                _page_max=int(re.findall('(\\d+)',_target_text)[1])\n",
    "            break\n",
    "\n",
    "\n",
    "    for p_idx in range(2,(_page_max+2)):\n",
    "        try:\n",
    "            if _page_soup.find('div')['id']==ERROR_404_CONSTANT:\n",
    "                break\n",
    "        except:\n",
    "                pass\n",
    "            \n",
    "        while True:\n",
    "            num_var=np.random.randint(100,1000)\n",
    "            num_var3=np.random.randint(10,100)\n",
    "            num_var2=num_var3%10\n",
    "            num_var4=np.random.randint(1000,10000)\n",
    "            num_var5=np.random.randint(100,1000)\n",
    "\n",
    "            header={\"User-Agent\": f\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/{num_var5}.36 (KHTML, like Gecko) Chrome/51.{num_var2}.2704.{num_var} Safari/537.{num_var3} OPR/38.0.{num_var4}.41\"}\n",
    "            req = requests.get(ZILLOW_HOMES_URL.format(city=city.lower(),\n",
    "                                                        state=state.lower(),\n",
    "                                                        property_type='homes',\n",
    "                                                        page=p_idx),headers=header)\n",
    "            \n",
    "            r_text=req.text\n",
    "\n",
    "\n",
    "            soup = BeautifulSoup(r_text,'html.parser')\n",
    "\n",
    "\n",
    "            if soup.find('meta')['name']==BOT_CAUGHT_CONSTANT:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "\n",
    "                homes = soup.find_all(HOME_HEADER_CLASS[0],HOME_HEADER_CLASS[1])\n",
    "\n",
    "                for home in homes:\n",
    "\n",
    "                    p_list=home.find(HOME_PRICE_CLASS[0],HOME_PRICE_CLASS[1]).text\n",
    "                    try:\n",
    "                        h['price']=int(\"\".join(re.findall('(\\d+)',p_list)))\n",
    "                    except:\n",
    "                        h['price']=None\n",
    "\n",
    "\n",
    "                    s_list=home.find(HOME_SPACE_CLASS[0],HOME_SPACE_CLASS[1])\n",
    "                    try:\n",
    "                        s_list_list = s_list.find_all('li')\n",
    "                        for li_idx in range(len(s_list_list)):\n",
    "                            li_identifier = s_list_list[li_idx].find('abbr')  \n",
    "                            try:\n",
    "                                if re.findall(\"(bd)\", li_identifier.text)[0] == 'bd': \n",
    "                                    h['bd'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                            try:\n",
    "                                if re.findall(\"(ba)\", li_identifier.text)[0] == 'ba':  \n",
    "                                    h['ba'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "                            try:\n",
    "                                if re.findall(\"(sqft)\", li_identifier.text)[0] == 'sqft':\n",
    "                                    h['sqft'] = int(\"\".join(re.findall('(\\d+)', s_list_list[li_idx].text)))\n",
    "                            except:\n",
    "                                pass\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    a_list=home.find(HOME_ADDRESS_CLASS[0],HOME_ADDRESS_CLASS[1]).text.split(\",\")\n",
    "                    try:\n",
    "                        h['address']=a_list[0].strip()\n",
    "                    except:\n",
    "                        h['address']=None\n",
    "                    try:\n",
    "                        h['city']=a_list[1].strip()\n",
    "                    except:\n",
    "                        h['city']=None\n",
    "                    try:\n",
    "                        h['state']=a_list[2].strip().split()[0]\n",
    "                    except:\n",
    "                        h['state']=None\n",
    "                    try:\n",
    "                        h['zip']=a_list[2].strip().split()[1]\n",
    "                    except:\n",
    "                        h['zip']=None\n",
    "  \n",
    "\n",
    "                    OW_api_call(h)\n",
    "\n",
    "                    l.append(h)\n",
    "                    h={}\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5999976312741637e-06\n"
     ]
    }
   ],
   "source": [
    "list=[]\n",
    "for range in (0,100):\n",
    "    tic=time.perf_counter()\n",
    "    OW_api_call(l[0])\n",
    "    toc=time.perf_counter()\n",
    "\n",
    "    list.append(toc-tic)\n",
    "\n",
    "print(mean(list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_urls(city_states,property_type:str,page_max_dict:dict=None):\n",
    "        urls=[]\n",
    "        for city,state in city_states:\n",
    "            try:\n",
    "                page_max=page_max_dict[(city,state)]\n",
    "            except:\n",
    "                page_max=1\n",
    "            for page in range(2,page_max+2):\n",
    "                urls.append(((city,state),ZILLOW_HOMES_URL.format(city=city.lower(),\n",
    "                                                    state=state.lower(),\n",
    "                                                    property_type=property_type,\n",
    "                                                    page=page)))\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_soup(self,city_state_tup,url):\n",
    "\n",
    "        while True:\n",
    "\n",
    "            header=self._randomized_UA()\n",
    "            \n",
    "            req=requests.get(url,\n",
    "                        headers=header)\n",
    "\n",
    "            req_text=req.text\n",
    "\n",
    "            soup=BeautifulSoup(req_text,'html.parser')\n",
    "\n",
    "            if self._check_503(req):\n",
    "                continue\n",
    "\n",
    "            if self._check_404(soup, city=city_state_tup[0],state=city_state_tup[1]):\n",
    "                self.city_states.remove((city_state_tup[0],city_state_tup[1]))\n",
    "                break\n",
    "\n",
    "            if self._check_robot(soup):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            break\n",
    "\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _threaded_request(self,func,urls):\n",
    "        soups=[]\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future_to_url = {executor.submit(func,city_state_tup,url):city_state_tup for (city_state_tup,url) in urls}\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                city_state_tup = future_to_url[future]\n",
    "                try:\n",
    "                    soups.append((city_state_tup,future.result()))\n",
    "                except:\n",
    "                    pass\n",
    "        return soups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "757b6ebce34926fd3f085c6fb2e5ec387b09d83a5ad4de95ec50463c150f98ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
